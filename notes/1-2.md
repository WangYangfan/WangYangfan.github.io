> Date: 2022/8/30
> 
> Author: WangYangfan
> 
> Paper Title: SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure
> 
> SGNN：一种基于图神经网络的隐藏结构的联邦学习方法

**背景**

图神经网络方法大多存在以下局限性

- 现有方法是基于网络中节点的内容，无法衡量节点的结构相似度

- 现有方法不能很好地保护用户隐私，包括原始内容信息和结构信息

**本文贡献**

- 重新定义节点对之间的距离，计算节点相似度来获取结构信息，用相似矩阵代替邻接矩阵更精确地描述节点之间的关系

- 利用联邦学习，尤其是垂直联邦学习的思想，隐藏用户的隐私信息。通过使用相似度矩阵，它隐藏了结构信息。此外，它使用 one-hot 编码方法隐藏内容信息

- 对不同数据集进行节点分类实验，包括大尺度图和小尺度图上进行学习。实验结果表明，SGNN 节点分类的精确度高于现有模型，尤其是在大规模图数据集上

**主要思想**

- 基于相似度，在节点分类中精确捕捉（远距离）节点的结构信息，使用具有卷积层和密集层的深度图神经网络根据节点的结构和特征对节点进行分类

- 利用联邦学习思想，保护用户原始数据隐私且不交换，各客户端协作学习训练模型

**相关概念**

Graph Neural Network (GNN)

- 图神经网络，使用神经网络来学习图结构数据，提出和发掘图结构数据中的特征和模式，满足聚类、分类、预测、分割、生成等图学习任务需求的算法总称
- 图神经网络划主要分为五大类别，分别是：图卷积网络（Graph Convolution Networks，GCN）、图注意力网络（Graph Attention Networks）、图自编码器（ Graph Autoencoders）、图生成网络（ Graph Generative Networks）和图时空网络（Graph Spatial-temporal Networks）

Network Embedding

- 网络嵌入，旨在学习具有高维特征和复杂结构的节点的低维表示

Federated Learning (FL)

- 联邦学习，一种分布式机器学习技术，在多个拥有本地数据的客户端之间进行分布式模型训练，在不需要交换本地原始数据的前提下，仅通过交换模型参数或中间结果的方式，共同协作学习训练模型

- 解决数据孤岛问题，保护各客户端原始数据的隐私，实现隐私保护与数据共享的平衡

**主要内容**

Introduction

- 图是具有节点属性和局部拓扑结构的数据的主要表示形式，一般来说，图数据包含数据的高维度的结构信息和内容信息

- 为了更容易表示和计算，已经提出了许多模型来降低图数据的维数。邻接矩阵虽然可以描述元素之间的直接联系，但无法描述非相邻节点之间的关系，现在已经提出了很多方法描述图的结构相似性而不是原始邻接矩阵，这些方法要么使用邻近度，要么使用层次结构衡量节点的结构相似性

- （随着深度学习的成功）介绍 GCN、GATs 等图神经网络方法，成为处理节点和图分类任务的主要方法，但是这些模型主要关注节点的邻域特征，无法衡量图中相距较远的节点的相似性

- 介绍联邦学习，保护用户原始数据隐私且不交换，不同数据来源的客户端协作学习训练模型

- 介绍本文的动机，一是精确捕捉远距离节点的结构信息，有助于完成分类任务；二是收集数据和模型训练过程中，应保护用户隐私。

- 介绍本文方法，重新定义节点之间的距离和计算节点的相似度，然后用相似矩阵代替邻接矩阵输入到图神经网络；原始内容信息、结构信息和标签信息来自三个不同数据源联邦学习（垂直联邦学习），原始内容信息加密，相似矩阵代替邻接矩阵隐藏内容信息，one-hot 编码矩阵代替原始特征矩阵来隐藏内容信息

- 主要贡献（SGNN 构造相似矩阵代替邻接矩阵输入、垂直联邦学习 one-hot 编码隐藏内容信息、大规模图实验上精度高）

Related Work

- 网络嵌入，旨在学习具有高维特征和复杂结构的节点的低维表示，以前大多数工作通过降低数据维度来计算节点表示，介绍 Word2vec、DeepWalk、LINE、GraRep、Struc2vec、Metapath2vec 等
- 图神经网络，在节点和图分类方面取得很大进展，介绍 GCN、GraphSAGE、GATs、DEMO-Net、P-GNN、LDS 等
- 联邦学习，一种隐私保护的多客户端协作训练的通用模型，包括横向（水平）联邦学习、纵向（垂直）联邦学习和联邦迁移学习
- 利用联邦学习的思想，利用图神经网络捕获节点相似性来分类网络中的节点，本文提出基于相似度的图结构神经网络，比现有模型更加注重结构信息，弥补图结构的损失，同时保留了特征空间的信息

Model: SGNN

- SGNN 从每个节点的局部领域获取结构信息来学习每个节点的表示，同时通过联邦学习隐藏内容信息和结构信息。首先，SGNN 对内容信息进行 one-hot 编码处理，然后，SGNN 计算节点之间的相似度，然后输入内容信息和结构信息，完成分类任务

- 考虑两种假设
  
  - 一是要保护用户隐私信息，用户的内容信息和结构信息可以分开表示，节点的嵌入不是仅仅包含自身特征，而且聚合其局部邻域的结构和特征信息（本文采用联邦学习框架，加密原始信息来保证用户隐私信息安全）
  
  - 二是两个节点之间的相似性与它们在结构上距离呈负相关。也就是说，局部结构越相似，其表示应该相似，而节点的局部结构差异越大，节点之间应该远离（本文通过计算节点之间的相似性来隐藏结构信息，而不是原始结构）

- SGNN 具体步骤
  
  - 计算节点对的相似性矩阵：计算节点的每个邻居的度，得到一个度的有序列表，然后使用 DTW 方法计算这些度列表的距离，表示节点之间的相似度。最后利用上述距离计算节点的相似度矩阵，距离越大的节点对权值越小 
    
    - $Distance(u,v)=DTW(s(N(u)),s(N(v)))$
    
    - $Similarity(u, v)=e^{-Distance(u,v)}$
  
  - 聚合特征向量（内容向量）：使用 one-hot 编码将原始特征映射到一个矩阵，矩阵每行表示用户特征
  
  - 训练图神经网络完成节点分类：输入处理过的内容信息和结构信息，通过增加密集层和使用包含度信息的权值矩阵训练网络来改进 GCN，以获得更好的节点分类效果
    
    - $h_v^k=\sigma (\sum_{u\in\{v\}\cup N(v)}\hat a_{vu}W^kh_u^{k-1})$
    
    - 为了弥补卷积层造成信息丢失，在原始模型增加全连接的密集层
  
  - 联邦学习体现在采用联邦学习模型，对数据的结构信息和内容信息分离，两种数据联邦训练图神经网络模型，从而对节点进行分类

**实验**

实验环境：对不同数据集进行节点分类实验，包括大尺度图和小尺度图。

实验结果：

- 节点分类的精确度高于现有模型，尤其是在大规模图数据集上
